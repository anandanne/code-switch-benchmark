{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8602852-254a-4343-a902-d7db3a12436e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    BertTokenizerFast,\n",
    "    AdamW,\n",
    "    get_scheduler\n",
    ")\n",
    "import torch\n",
    "\n",
    "from model import BertPromptTuningLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8352998f-c2c8-4968-9f69-26cdd2cb51a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # Same default parameters as run_clm_no_trainer.py in tranformers\n",
    "    # https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_clm_no_trainer.py\n",
    "    num_train_epochs = 3\n",
    "    weight_decay = 0.01\n",
    "    learning_rate = 0.01\n",
    "    lr_scheduler_type = \"linear\"\n",
    "    num_warmup_steps = 0\n",
    "    max_train_steps = num_train_epochs\n",
    "    \n",
    "    # Prompt-tuning\n",
    "    # number of prompt tokens\n",
    "    n_prompt_tokens = 20\n",
    "    # If True, soft prompt will be initialized from vocab \n",
    "    # Otherwise, you can set `random_range` to initialize by randomization.\n",
    "    init_from_vocab = False\n",
    "    # random_range = 0.5\n",
    "args = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9806ce87-7cbe-4f3a-93a7-b3fb44ec531d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertPromptTuningLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertPromptTuningLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertPromptTuningLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing soft prompt...\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "# Initialize GPT2LM with soft prompt\n",
    "model = BertPromptTuningLM.from_pretrained(\n",
    "    \"bert-base-multilingual-cased\",\n",
    "    n_tokens=args.n_prompt_tokens,\n",
    "    initialize_from_vocab=args.init_from_vocab\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb8f9fe5-a40a-4e74-93d0-8f727c0c4f92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 768])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.soft_prompt.weight\n",
    "model.soft_prompt.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27d1b6ba-ad76-47ca-b9d8-b0ba7fe2b7ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101, 31178,   117, 15127, 17835, 10124, 21610, 10112,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "530aaaad-a62f-4934-8017-375ce8df6cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only update soft prompt'weights for prompt-tuning. ie, all weights in LM are set as `require_grad=False`. \n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if n == \"soft_prompt.weight\"],\n",
    "        \"weight_decay\": args.weight_decay,\n",
    "    }\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate)\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=args.lr_scheduler_type,\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=args.num_warmup_steps,\n",
    "    num_training_steps=args.max_train_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19dfa2a8-d211-4cbf-bd26-f130f2dff0f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 11.808565139770508\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "loss = outputs.loss\n",
    "print(f\"loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5d69c1e-519e-4134-b3a6-dcab61f5431c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ca1385d-d5cf-4151-bcd9-72ab2440a73f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0159, -0.0162,  0.0059,  ...,  0.0197,  0.0142,  0.0097],\n",
       "        [ 0.0204, -0.0236, -0.0033,  ...,  0.0024,  0.0167,  0.0237],\n",
       "        [ 0.0321, -0.0136,  0.0093,  ...,  0.0169,  0.0368, -0.0050],\n",
       "        ...,\n",
       "        [ 0.0084,  0.0064,  0.0040,  ...,  0.0277, -0.0045,  0.0214],\n",
       "        [ 0.0283,  0.0088, -0.0009,  ...,  0.0460,  0.0362,  0.0307],\n",
       "        [ 0.0178, -0.0307, -0.0045,  ...,  0.0190, -0.0036,  0.0017]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.soft_prompt.weight\n",
    "# Confirmed the weights were changed! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868551eb-96df-41aa-9e65-119d81a76f16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
